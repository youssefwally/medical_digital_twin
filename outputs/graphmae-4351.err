wandb: Currently logged in as: yussufwaly. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /u/home/wyo/digital_twin/scripts/wandb/run-20230711_080740-oaj1mjao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-plant-52
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yussufwaly/digital_twin_graphmae
wandb: üöÄ View run at https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/oaj1mjao
./GraphMAE/graphmae.py:128: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
INFO - 2023-07-11 08:38:10,114 - graphmae - start training..
  0%|          | 0/20 [00:00<?, ?it/s]# Epoch 0 | train_loss: 0.0334 | val_loss: 0.0195 | test_loss: 0.0195:   0%|          | 0/20 [10:17<?, ?it/s]# Epoch 0 | train_loss: 0.0334 | val_loss: 0.0195 | test_loss: 0.0195:   5%|‚ñå         | 1/20 [10:17<3:15:29, 617.34s/it]# Epoch 1 | train_loss: 0.0160 | val_loss: 0.0122 | test_loss: 0.0122:   5%|‚ñå         | 1/20 [20:36<3:15:29, 617.34s/it]# Epoch 1 | train_loss: 0.0160 | val_loss: 0.0122 | test_loss: 0.0122:  10%|‚ñà         | 2/20 [20:36<3:05:31, 618.40s/it]# Epoch 2 | train_loss: 0.0109 | val_loss: 0.0088 | test_loss: 0.0088:  10%|‚ñà         | 2/20 [30:57<3:05:31, 618.40s/it]# Epoch 2 | train_loss: 0.0109 | val_loss: 0.0088 | test_loss: 0.0088:  15%|‚ñà‚ñå        | 3/20 [30:57<2:55:31, 619.50s/it]# Epoch 3 | train_loss: 0.0080 | val_loss: 0.0066 | test_loss: 0.0066:  15%|‚ñà‚ñå        | 3/20 [41:01<2:55:31, 619.50s/it]# Epoch 3 | train_loss: 0.0080 | val_loss: 0.0066 | test_loss: 0.0066:  20%|‚ñà‚ñà        | 4/20 [41:01<2:43:37, 613.60s/it]# Epoch 4 | train_loss: 0.0061 | val_loss: 0.0051 | test_loss: 0.0050:  20%|‚ñà‚ñà        | 4/20 [51:24<2:43:37, 613.60s/it]# Epoch 4 | train_loss: 0.0061 | val_loss: 0.0051 | test_loss: 0.0050:  25%|‚ñà‚ñà‚ñå       | 5/20 [51:24<2:34:11, 616.75s/it]# Epoch 5 | train_loss: 0.0047 | val_loss: 0.0039 | test_loss: 0.0039:  25%|‚ñà‚ñà‚ñå       | 5/20 [1:01:47<2:34:11, 616.75s/it]# Epoch 5 | train_loss: 0.0047 | val_loss: 0.0039 | test_loss: 0.0039:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:01:47<2:24:27, 619.10s/it]# Epoch 6 | train_loss: 0.0036 | val_loss: 0.0030 | test_loss: 0.0030:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:12:11<2:24:27, 619.10s/it]# Epoch 6 | train_loss: 0.0036 | val_loss: 0.0030 | test_loss: 0.0030:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:12:11<2:14:27, 620.58s/it]# Epoch 7 | train_loss: 0.0028 | val_loss: 0.0023 | test_loss: 0.0023:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:22:34<2:14:27, 620.58s/it]# Epoch 7 | train_loss: 0.0028 | val_loss: 0.0023 | test_loss: 0.0023:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:22:34<2:04:16, 621.38s/it]# Epoch 8 | train_loss: 0.0022 | val_loss: 0.0018 | test_loss: 0.0018:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:33:07<2:04:16, 621.38s/it]# Epoch 8 | train_loss: 0.0022 | val_loss: 0.0018 | test_loss: 0.0018:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:33:07<1:54:35, 625.06s/it]# Epoch 9 | train_loss: 0.0017 | val_loss: 0.0014 | test_loss: 0.0014:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:43:36<1:54:35, 625.06s/it]# Epoch 9 | train_loss: 0.0017 | val_loss: 0.0014 | test_loss: 0.0014:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:43:36<1:44:22, 626.21s/it]# Epoch 10 | train_loss: 0.0014 | val_loss: 0.0012 | test_loss: 0.0012:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:54:03<1:44:22, 626.21s/it]# Epoch 10 | train_loss: 0.0014 | val_loss: 0.0012 | test_loss: 0.0012:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [1:54:03<1:33:57, 626.39s/it]# Epoch 11 | train_loss: 0.0012 | val_loss: 0.0010 | test_loss: 0.0010:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:04:29<1:33:57, 626.39s/it]# Epoch 11 | train_loss: 0.0012 | val_loss: 0.0010 | test_loss: 0.0010:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:04:29<1:23:31, 626.45s/it]# Epoch 12 | train_loss: 0.0010 | val_loss: 0.0009 | test_loss: 0.0009:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:14:56<1:23:31, 626.45s/it]# Epoch 12 | train_loss: 0.0010 | val_loss: 0.0009 | test_loss: 0.0009:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:14:56<1:13:05, 626.48s/it]# Epoch 13 | train_loss: 0.0009 | val_loss: 0.0008 | test_loss: 0.0008:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:25:24<1:13:05, 626.48s/it]# Epoch 13 | train_loss: 0.0009 | val_loss: 0.0008 | test_loss: 0.0008:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:25:24<1:02:41, 626.94s/it]# Epoch 14 | train_loss: 0.0009 | val_loss: 0.0008 | test_loss: 0.0008:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:35:53<1:02:41, 626.94s/it]# Epoch 14 | train_loss: 0.0009 | val_loss: 0.0008 | test_loss: 0.0008:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:35:53<52:17, 627.44s/it]  # Epoch 15 | train_loss: 0.0008 | val_loss: 0.0008 | test_loss: 0.0008:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:46:22<52:17, 627.44s/it]# Epoch 15 | train_loss: 0.0008 | val_loss: 0.0008 | test_loss: 0.0008:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [2:46:22<41:52, 628.05s/it]# Epoch 16 | train_loss: 0.0008 | val_loss: 0.0008 | test_loss: 0.0008:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [2:56:51<41:52, 628.05s/it]# Epoch 16 | train_loss: 0.0008 | val_loss: 0.0008 | test_loss: 0.0008:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [2:56:51<31:24, 628.19s/it]# Epoch 17 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:07:22<31:24, 628.19s/it]# Epoch 17 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:07:22<20:58, 629.09s/it]# Epoch 18 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:17:53<20:58, 629.09s/it]# Epoch 18 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:17:53<10:29, 629.61s/it]# Epoch 19 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:28:24<10:29, 629.61s/it]# Epoch 19 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:28:24<00:00, 630.31s/it]# Epoch 19 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:28:24<00:00, 625.25s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:  test_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 19
wandb:  test_loss 0.00072
wandb: train_loss 0.00078
wandb:   val_loss 0.00073
wandb: 
wandb: üöÄ View run rich-plant-52 at: https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/oaj1mjao
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230711_080740-oaj1mjao/logs

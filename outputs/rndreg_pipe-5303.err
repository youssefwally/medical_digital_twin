wandb: Currently logged in as: yussufwaly. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /u/home/wyo/digital_twin/scripts/wandb/run-20230723_160739-i5lnxw4f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-tree-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yussufwaly/digital_twin_graphmae
wandb: üöÄ View run at https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/i5lnxw4f
./GraphMAE/graphmae.py:128: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
INFO - 2023-07-23 16:49:56,578 - graphmae - start training..
  0%|          | 0/20 [00:00<?, ?it/s]# Epoch 0 | train_loss: 0.0284 | val_loss: 0.0124 | test_loss: 0.0123:   0%|          | 0/20 [10:18<?, ?it/s]# Epoch 0 | train_loss: 0.0284 | val_loss: 0.0124 | test_loss: 0.0123:   5%|‚ñå         | 1/20 [10:18<3:15:55, 618.71s/it]# Epoch 1 | train_loss: 0.0091 | val_loss: 0.0063 | test_loss: 0.0063:   5%|‚ñå         | 1/20 [21:25<3:15:55, 618.71s/it]# Epoch 1 | train_loss: 0.0091 | val_loss: 0.0063 | test_loss: 0.0063:  10%|‚ñà         | 2/20 [21:25<3:14:10, 647.25s/it]# Epoch 2 | train_loss: 0.0052 | val_loss: 0.0039 | test_loss: 0.0039:  10%|‚ñà         | 2/20 [32:42<3:14:10, 647.25s/it]# Epoch 2 | train_loss: 0.0052 | val_loss: 0.0039 | test_loss: 0.0039:  15%|‚ñà‚ñå        | 3/20 [32:42<3:07:11, 660.70s/it]# Epoch 3 | train_loss: 0.0034 | val_loss: 0.0027 | test_loss: 0.0027:  15%|‚ñà‚ñå        | 3/20 [43:53<3:07:11, 660.70s/it]# Epoch 3 | train_loss: 0.0034 | val_loss: 0.0027 | test_loss: 0.0027:  20%|‚ñà‚ñà        | 4/20 [43:53<2:57:12, 664.54s/it]# Epoch 4 | train_loss: 0.0024 | val_loss: 0.0019 | test_loss: 0.0019:  20%|‚ñà‚ñà        | 4/20 [55:08<2:57:12, 664.54s/it]# Epoch 4 | train_loss: 0.0024 | val_loss: 0.0019 | test_loss: 0.0019:  25%|‚ñà‚ñà‚ñå       | 5/20 [55:08<2:47:04, 668.33s/it]# Epoch 5 | train_loss: 0.0017 | val_loss: 0.0014 | test_loss: 0.0014:  25%|‚ñà‚ñà‚ñå       | 5/20 [1:06:25<2:47:04, 668.33s/it]# Epoch 5 | train_loss: 0.0017 | val_loss: 0.0014 | test_loss: 0.0014:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:06:25<2:36:38, 671.34s/it]# Epoch 6 | train_loss: 0.0013 | val_loss: 0.0011 | test_loss: 0.0011:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:18:45<2:36:38, 671.34s/it]# Epoch 6 | train_loss: 0.0013 | val_loss: 0.0011 | test_loss: 0.0011:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:18:45<2:30:18, 693.75s/it]# Epoch 7 | train_loss: 0.0011 | val_loss: 0.0009 | test_loss: 0.0009:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:30:32<2:30:18, 693.75s/it]# Epoch 7 | train_loss: 0.0011 | val_loss: 0.0009 | test_loss: 0.0009:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:30:32<2:19:37, 698.11s/it]# Epoch 8 | train_loss: 0.0009 | val_loss: 0.0008 | test_loss: 0.0008:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:42:16<2:19:37, 698.11s/it]# Epoch 8 | train_loss: 0.0009 | val_loss: 0.0008 | test_loss: 0.0008:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:42:16<2:08:20, 700.01s/it]# Epoch 9 | train_loss: 0.0008 | val_loss: 0.0008 | test_loss: 0.0008:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:53:45<2:08:20, 700.01s/it]# Epoch 9 | train_loss: 0.0008 | val_loss: 0.0008 | test_loss: 0.0008:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:53:45<1:56:04, 696.44s/it]# Epoch 10 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [2:05:36<1:56:04, 696.44s/it]# Epoch 10 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:05:36<1:45:08, 700.96s/it]# Epoch 11 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:17:37<1:45:08, 700.96s/it]# Epoch 11 | train_loss: 0.0008 | val_loss: 0.0007 | test_loss: 0.0007:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:17:37<1:34:16, 707.10s/it]# Epoch 12 | train_loss: 0.0007 | val_loss: 0.0007 | test_loss: 0.0007:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:29:32<1:34:16, 707.10s/it]# Epoch 12 | train_loss: 0.0007 | val_loss: 0.0007 | test_loss: 0.0007:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:29:32<1:22:45, 709.33s/it]# Epoch 13 | train_loss: 0.0007 | val_loss: 0.0007 | test_loss: 0.0007:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:41:02<1:22:45, 709.33s/it]# Epoch 13 | train_loss: 0.0007 | val_loss: 0.0007 | test_loss: 0.0007:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:41:02<1:10:21, 703.56s/it]# Epoch 14 | train_loss: 0.0007 | val_loss: 0.0007 | test_loss: 0.0007:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:52:22<1:10:21, 703.56s/it]# Epoch 14 | train_loss: 0.0007 | val_loss: 0.0007 | test_loss: 0.0007:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:52:22<58:01, 696.38s/it]  # Epoch 15 | train_loss: 0.0007 | val_loss: 0.0007 | test_loss: 0.0006:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [3:03:42<58:01, 696.38s/it]# Epoch 15 | train_loss: 0.0007 | val_loss: 0.0007 | test_loss: 0.0006:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [3:03:42<46:06, 691.72s/it]# Epoch 16 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [3:15:20<46:06, 691.72s/it]# Epoch 16 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:15:20<34:40, 693.52s/it]# Epoch 17 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:26:36<34:40, 693.52s/it]# Epoch 17 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:26:36<22:56, 688.30s/it]# Epoch 18 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:38:07<22:56, 688.30s/it]# Epoch 18 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:38:07<11:29, 689.06s/it]# Epoch 19 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:49:32<11:29, 689.06s/it]# Epoch 19 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:49:32<00:00, 687.85s/it]# Epoch 19 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:49:32<00:00, 688.63s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:  test_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 19
wandb:  test_loss 0.00063
wandb: train_loss 0.00068
wandb:   val_loss 0.00063
wandb: 
wandb: üöÄ View run vibrant-tree-55 at: https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/i5lnxw4f
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230723_160739-i5lnxw4f/logs
wandb: Currently logged in as: yussufwaly. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /u/home/wyo/digital_twin/scripts/wandb/run-20230723_203959-m4zqfa53
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-firefly-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yussufwaly/digital_twin_graphmae
wandb: üöÄ View run at https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/m4zqfa53
./GraphMAE/graphmae.py:128: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
INFO - 2023-07-23 21:26:29,179 - graphmae - start training..
  0%|          | 0/20 [00:00<?, ?it/s]# Epoch 0 | train_loss: 0.0510 | val_loss: 0.0183 | test_loss: 0.0183:   0%|          | 0/20 [10:34<?, ?it/s]# Epoch 0 | train_loss: 0.0510 | val_loss: 0.0183 | test_loss: 0.0183:   5%|‚ñå         | 1/20 [10:34<3:21:03, 634.93s/it]# Epoch 1 | train_loss: 0.0117 | val_loss: 0.0070 | test_loss: 0.0070:   5%|‚ñå         | 1/20 [20:51<3:21:03, 634.93s/it]# Epoch 1 | train_loss: 0.0117 | val_loss: 0.0070 | test_loss: 0.0070:  10%|‚ñà         | 2/20 [20:51<3:07:18, 624.35s/it]# Epoch 2 | train_loss: 0.0055 | val_loss: 0.0039 | test_loss: 0.0038:  10%|‚ñà         | 2/20 [30:40<3:07:18, 624.35s/it]# Epoch 2 | train_loss: 0.0055 | val_loss: 0.0039 | test_loss: 0.0038:  15%|‚ñà‚ñå        | 3/20 [30:40<2:52:14, 607.92s/it]# Epoch 3 | train_loss: 0.0032 | val_loss: 0.0024 | test_loss: 0.0024:  15%|‚ñà‚ñå        | 3/20 [41:02<2:52:14, 607.92s/it]# Epoch 3 | train_loss: 0.0032 | val_loss: 0.0024 | test_loss: 0.0024:  20%|‚ñà‚ñà        | 4/20 [41:02<2:43:37, 613.57s/it]# Epoch 4 | train_loss: 0.0020 | val_loss: 0.0016 | test_loss: 0.0016:  20%|‚ñà‚ñà        | 4/20 [52:42<2:43:37, 613.57s/it]# Epoch 4 | train_loss: 0.0020 | val_loss: 0.0016 | test_loss: 0.0016:  25%|‚ñà‚ñà‚ñå       | 5/20 [52:42<2:41:08, 644.60s/it]# Epoch 5 | train_loss: 0.0014 | val_loss: 0.0011 | test_loss: 0.0011:  25%|‚ñà‚ñà‚ñå       | 5/20 [1:03:38<2:41:08, 644.60s/it]# Epoch 5 | train_loss: 0.0014 | val_loss: 0.0011 | test_loss: 0.0011:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:03:38<2:31:21, 648.69s/it]# Epoch 6 | train_loss: 0.0010 | val_loss: 0.0008 | test_loss: 0.0008:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:14:28<2:31:21, 648.69s/it]# Epoch 6 | train_loss: 0.0010 | val_loss: 0.0008 | test_loss: 0.0008:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:14:28<2:20:36, 648.95s/it]# Epoch 7 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:25:19<2:20:36, 648.95s/it]# Epoch 7 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:25:19<2:09:56, 649.71s/it]# Epoch 8 | train_loss: 0.0006 | val_loss: 0.0005 | test_loss: 0.0005:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:36:08<2:09:56, 649.71s/it]# Epoch 8 | train_loss: 0.0006 | val_loss: 0.0005 | test_loss: 0.0005:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:36:08<1:59:05, 649.62s/it]# Epoch 9 | train_loss: 0.0005 | val_loss: 0.0004 | test_loss: 0.0004:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:46:44<1:59:05, 649.62s/it]# Epoch 9 | train_loss: 0.0005 | val_loss: 0.0004 | test_loss: 0.0004:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:46:44<1:47:31, 645.12s/it]# Epoch 10 | train_loss: 0.0004 | val_loss: 0.0004 | test_loss: 0.0004:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:58:07<1:47:31, 645.12s/it]# Epoch 10 | train_loss: 0.0004 | val_loss: 0.0004 | test_loss: 0.0004:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [1:58:07<1:38:31, 656.82s/it]# Epoch 11 | train_loss: 0.0004 | val_loss: 0.0003 | test_loss: 0.0003:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:09:38<1:38:31, 656.82s/it]# Epoch 11 | train_loss: 0.0004 | val_loss: 0.0003 | test_loss: 0.0003:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:09:38<1:28:58, 667.27s/it]# Epoch 12 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:20:41<1:28:58, 667.27s/it]# Epoch 12 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:20:41<1:17:40, 665.82s/it]# Epoch 13 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:31:37<1:17:40, 665.82s/it]# Epoch 13 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:31:37<1:06:17, 662.92s/it]# Epoch 14 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:42:57<1:06:17, 662.92s/it]# Epoch 14 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:42:57<55:41, 668.28s/it]  # Epoch 15 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:54:28<55:41, 668.28s/it]# Epoch 15 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [2:54:28<45:00, 675.12s/it]# Epoch 16 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [3:05:27<45:00, 675.12s/it]# Epoch 16 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:05:27<33:30, 670.26s/it]# Epoch 17 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:16:22<33:30, 670.26s/it]# Epoch 17 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:16:22<22:11, 665.68s/it]# Epoch 18 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:27:33<22:11, 665.68s/it]# Epoch 18 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:27:33<11:07, 667.13s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:39:06<11:07, 667.13s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:39:06<00:00, 674.89s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:39:06<00:00, 657.32s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:  test_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 19
wandb:  test_loss 0.0002
wandb: train_loss 0.00022
wandb:   val_loss 0.0002
wandb: 
wandb: üöÄ View run serene-firefly-56 at: https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/m4zqfa53
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230723_203959-m4zqfa53/logs
wandb: Currently logged in as: yussufwaly. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /u/home/wyo/digital_twin/scripts/wandb/run-20230724_010605-zdqgumya
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-dust-57
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yussufwaly/digital_twin_graphmae
wandb: üöÄ View run at https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/zdqgumya
./GraphMAE/graphmae.py:128: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
INFO - 2023-07-24 01:56:03,967 - graphmae - start training..
  0%|          | 0/20 [00:00<?, ?it/s]# Epoch 0 | train_loss: 0.0314 | val_loss: 0.0109 | test_loss: 0.0109:   0%|          | 0/20 [10:44<?, ?it/s]# Epoch 0 | train_loss: 0.0314 | val_loss: 0.0109 | test_loss: 0.0109:   5%|‚ñå         | 1/20 [10:44<3:23:58, 644.14s/it]# Epoch 1 | train_loss: 0.0073 | val_loss: 0.0045 | test_loss: 0.0045:   5%|‚ñå         | 1/20 [21:34<3:23:58, 644.14s/it]# Epoch 1 | train_loss: 0.0073 | val_loss: 0.0045 | test_loss: 0.0045:  10%|‚ñà         | 2/20 [21:34<3:14:25, 648.08s/it]# Epoch 2 | train_loss: 0.0037 | val_loss: 0.0026 | test_loss: 0.0026:  10%|‚ñà         | 2/20 [32:36<3:14:25, 648.08s/it]# Epoch 2 | train_loss: 0.0037 | val_loss: 0.0026 | test_loss: 0.0026:  15%|‚ñà‚ñå        | 3/20 [32:36<3:05:24, 654.40s/it]# Epoch 3 | train_loss: 0.0023 | val_loss: 0.0017 | test_loss: 0.0017:  15%|‚ñà‚ñå        | 3/20 [43:39<3:05:24, 654.40s/it]# Epoch 3 | train_loss: 0.0023 | val_loss: 0.0017 | test_loss: 0.0017:  20%|‚ñà‚ñà        | 4/20 [43:39<2:55:22, 657.67s/it]# Epoch 4 | train_loss: 0.0015 | val_loss: 0.0011 | test_loss: 0.0011:  20%|‚ñà‚ñà        | 4/20 [54:56<2:55:22, 657.67s/it]# Epoch 4 | train_loss: 0.0015 | val_loss: 0.0011 | test_loss: 0.0011:  25%|‚ñà‚ñà‚ñå       | 5/20 [54:56<2:46:08, 664.54s/it]# Epoch 5 | train_loss: 0.0010 | val_loss: 0.0008 | test_loss: 0.0008:  25%|‚ñà‚ñà‚ñå       | 5/20 [1:06:09<2:46:08, 664.54s/it]# Epoch 5 | train_loss: 0.0010 | val_loss: 0.0008 | test_loss: 0.0008:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:06:09<2:35:46, 667.64s/it]# Epoch 6 | train_loss: 0.0008 | val_loss: 0.0006 | test_loss: 0.0006:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:17:31<2:35:46, 667.64s/it]# Epoch 6 | train_loss: 0.0008 | val_loss: 0.0006 | test_loss: 0.0006:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:17:31<2:25:38, 672.15s/it]# Epoch 7 | train_loss: 0.0006 | val_loss: 0.0005 | test_loss: 0.0005:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:28:36<2:25:38, 672.15s/it]# Epoch 7 | train_loss: 0.0006 | val_loss: 0.0005 | test_loss: 0.0005:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:28:36<2:13:58, 669.87s/it]# Epoch 8 | train_loss: 0.0005 | val_loss: 0.0004 | test_loss: 0.0004:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:39:30<2:13:58, 669.87s/it]# Epoch 8 | train_loss: 0.0005 | val_loss: 0.0004 | test_loss: 0.0004:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:39:30<2:01:53, 664.87s/it]# Epoch 9 | train_loss: 0.0004 | val_loss: 0.0004 | test_loss: 0.0004:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:50:38<2:01:53, 664.87s/it]# Epoch 9 | train_loss: 0.0004 | val_loss: 0.0004 | test_loss: 0.0004:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:50:38<1:50:57, 665.77s/it]# Epoch 10 | train_loss: 0.0004 | val_loss: 0.0003 | test_loss: 0.0003:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [2:02:03<1:50:57, 665.77s/it]# Epoch 10 | train_loss: 0.0004 | val_loss: 0.0003 | test_loss: 0.0003:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:02:03<1:40:45, 671.74s/it]# Epoch 11 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:13:13<1:40:45, 671.74s/it]# Epoch 11 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:13:13<1:29:30, 671.27s/it]# Epoch 12 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:24:07<1:29:30, 671.27s/it]# Epoch 12 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:24:07<1:17:42, 666.00s/it]# Epoch 13 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:34:58<1:17:42, 666.00s/it]# Epoch 13 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:34:58<1:06:09, 661.52s/it]# Epoch 14 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:46:30<1:06:09, 661.52s/it]# Epoch 14 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:46:30<55:54, 670.81s/it]  # Epoch 15 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:57:48<55:54, 670.81s/it]# Epoch 15 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [2:57:48<44:52, 673.00s/it]# Epoch 16 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [3:08:53<44:52, 673.00s/it]# Epoch 16 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:08:53<33:31, 670.42s/it]# Epoch 17 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:19:45<33:31, 670.42s/it]# Epoch 17 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:19:45<22:09, 664.86s/it]# Epoch 18 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:31:08<22:09, 664.86s/it]# Epoch 18 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:31:08<11:10, 670.49s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:42:39<11:10, 670.49s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:42:39<00:00, 676.65s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:42:39<00:00, 668.00s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:  test_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 19
wandb:  test_loss 0.00021
wandb: train_loss 0.00023
wandb:   val_loss 0.00021
wandb: 
wandb: üöÄ View run royal-dust-57 at: https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/zdqgumya
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230724_010605-zdqgumya/logs
wandb: Currently logged in as: yussufwaly. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /u/home/wyo/digital_twin/scripts/wandb/run-20230724_053948-7ezi9096
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-glitter-58
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yussufwaly/digital_twin_graphmae
wandb: üöÄ View run at https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/7ezi9096
./GraphMAE/graphmae.py:128: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
INFO - 2023-07-24 06:25:01,874 - graphmae - start training..
  0%|          | 0/20 [00:00<?, ?it/s]# Epoch 0 | train_loss: 0.0408 | val_loss: 0.0143 | test_loss: 0.0143:   0%|          | 0/20 [10:39<?, ?it/s]# Epoch 0 | train_loss: 0.0408 | val_loss: 0.0143 | test_loss: 0.0143:   5%|‚ñå         | 1/20 [10:39<3:22:33, 639.65s/it]# Epoch 1 | train_loss: 0.0093 | val_loss: 0.0057 | test_loss: 0.0057:   5%|‚ñå         | 1/20 [21:31<3:22:33, 639.65s/it]# Epoch 1 | train_loss: 0.0093 | val_loss: 0.0057 | test_loss: 0.0057:  10%|‚ñà         | 2/20 [21:31<3:13:59, 646.66s/it]# Epoch 2 | train_loss: 0.0046 | val_loss: 0.0033 | test_loss: 0.0033:  10%|‚ñà         | 2/20 [32:48<3:13:59, 646.66s/it]# Epoch 2 | train_loss: 0.0046 | val_loss: 0.0033 | test_loss: 0.0033:  15%|‚ñà‚ñå        | 3/20 [32:48<3:07:11, 660.70s/it]# Epoch 3 | train_loss: 0.0028 | val_loss: 0.0021 | test_loss: 0.0021:  15%|‚ñà‚ñå        | 3/20 [44:05<3:07:11, 660.70s/it]# Epoch 3 | train_loss: 0.0028 | val_loss: 0.0021 | test_loss: 0.0021:  20%|‚ñà‚ñà        | 4/20 [44:05<2:57:55, 667.20s/it]# Epoch 4 | train_loss: 0.0019 | val_loss: 0.0014 | test_loss: 0.0014:  20%|‚ñà‚ñà        | 4/20 [54:47<2:57:55, 667.20s/it]# Epoch 4 | train_loss: 0.0019 | val_loss: 0.0014 | test_loss: 0.0014:  25%|‚ñà‚ñà‚ñå       | 5/20 [54:47<2:44:27, 657.83s/it]# Epoch 5 | train_loss: 0.0013 | val_loss: 0.0010 | test_loss: 0.0010:  25%|‚ñà‚ñà‚ñå       | 5/20 [1:05:28<2:44:27, 657.83s/it]# Epoch 5 | train_loss: 0.0013 | val_loss: 0.0010 | test_loss: 0.0010:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:05:28<2:32:13, 652.39s/it]# Epoch 6 | train_loss: 0.0009 | val_loss: 0.0008 | test_loss: 0.0008:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:16:08<2:32:13, 652.39s/it]# Epoch 6 | train_loss: 0.0009 | val_loss: 0.0008 | test_loss: 0.0008:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:16:08<2:20:28, 648.37s/it]# Epoch 7 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:26:57<2:20:28, 648.37s/it]# Epoch 7 | train_loss: 0.0007 | val_loss: 0.0006 | test_loss: 0.0006:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:26:57<2:09:43, 648.59s/it]# Epoch 8 | train_loss: 0.0006 | val_loss: 0.0005 | test_loss: 0.0005:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:37:20<2:09:43, 648.59s/it]# Epoch 8 | train_loss: 0.0006 | val_loss: 0.0005 | test_loss: 0.0005:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:37:20<1:57:23, 640.34s/it]# Epoch 9 | train_loss: 0.0005 | val_loss: 0.0004 | test_loss: 0.0004:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:47:04<1:57:23, 640.34s/it]# Epoch 9 | train_loss: 0.0005 | val_loss: 0.0004 | test_loss: 0.0004:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:47:04<1:43:50, 623.01s/it]# Epoch 10 | train_loss: 0.0004 | val_loss: 0.0003 | test_loss: 0.0003:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:57:48<1:43:50, 623.01s/it]# Epoch 10 | train_loss: 0.0004 | val_loss: 0.0003 | test_loss: 0.0003:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [1:57:48<1:34:25, 629.46s/it]# Epoch 11 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:08:40<1:34:25, 629.46s/it]# Epoch 11 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:08:40<1:24:50, 636.29s/it]# Epoch 12 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:19:17<1:24:50, 636.29s/it]# Epoch 12 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:19:17<1:14:16, 636.58s/it]# Epoch 13 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:29:54<1:14:16, 636.58s/it]# Epoch 13 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:29:54<1:03:39, 636.53s/it]# Epoch 14 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:40:29<1:03:39, 636.53s/it]# Epoch 14 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:40:29<53:01, 636.35s/it]  # Epoch 15 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:50:21<53:01, 636.35s/it]# Epoch 15 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [2:50:21<41:31, 622.78s/it]# Epoch 16 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [3:02:07<41:31, 622.78s/it]# Epoch 16 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:02:07<32:23, 647.97s/it]# Epoch 17 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:13:46<32:23, 647.97s/it]# Epoch 17 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:13:46<22:06, 663.24s/it]# Epoch 18 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:24:17<22:06, 663.24s/it]# Epoch 18 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:24:17<10:53, 653.50s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:35:00<10:53, 653.50s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:35:00<00:00, 650.41s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:35:00<00:00, 645.03s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:  test_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 19
wandb:  test_loss 0.00017
wandb: train_loss 0.00019
wandb:   val_loss 0.00017
wandb: 
wandb: üöÄ View run bright-glitter-58 at: https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/7ezi9096
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230724_053948-7ezi9096/logs
wandb: Currently logged in as: yussufwaly. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /u/home/wyo/digital_twin/scripts/wandb/run-20230724_100033-worj21bc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-dew-59
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yussufwaly/digital_twin_graphmae
wandb: üöÄ View run at https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/worj21bc
./GraphMAE/graphmae.py:128: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
INFO - 2023-07-24 10:42:13,270 - graphmae - start training..
  0%|          | 0/20 [00:00<?, ?it/s]# Epoch 0 | train_loss: 0.0223 | val_loss: 0.0102 | test_loss: 0.0102:   0%|          | 0/20 [10:06<?, ?it/s]# Epoch 0 | train_loss: 0.0223 | val_loss: 0.0102 | test_loss: 0.0102:   5%|‚ñå         | 1/20 [10:06<3:12:05, 606.62s/it]# Epoch 1 | train_loss: 0.0073 | val_loss: 0.0046 | test_loss: 0.0046:   5%|‚ñå         | 1/20 [20:35<3:12:05, 606.62s/it]# Epoch 1 | train_loss: 0.0073 | val_loss: 0.0046 | test_loss: 0.0046:  10%|‚ñà         | 2/20 [20:35<3:05:58, 619.90s/it]# Epoch 2 | train_loss: 0.0036 | val_loss: 0.0024 | test_loss: 0.0024:  10%|‚ñà         | 2/20 [31:07<3:05:58, 619.90s/it]# Epoch 2 | train_loss: 0.0036 | val_loss: 0.0024 | test_loss: 0.0024:  15%|‚ñà‚ñå        | 3/20 [31:07<2:57:13, 625.48s/it]# Epoch 3 | train_loss: 0.0020 | val_loss: 0.0014 | test_loss: 0.0014:  15%|‚ñà‚ñå        | 3/20 [41:39<2:57:13, 625.48s/it]# Epoch 3 | train_loss: 0.0020 | val_loss: 0.0014 | test_loss: 0.0014:  20%|‚ñà‚ñà        | 4/20 [41:39<2:47:24, 627.77s/it]# Epoch 4 | train_loss: 0.0012 | val_loss: 0.0009 | test_loss: 0.0009:  20%|‚ñà‚ñà        | 4/20 [51:58<2:47:24, 627.77s/it]# Epoch 4 | train_loss: 0.0012 | val_loss: 0.0009 | test_loss: 0.0009:  25%|‚ñà‚ñà‚ñå       | 5/20 [51:58<2:36:11, 624.78s/it]# Epoch 5 | train_loss: 0.0008 | val_loss: 0.0006 | test_loss: 0.0006:  25%|‚ñà‚ñà‚ñå       | 5/20 [1:02:17<2:36:11, 624.78s/it]# Epoch 5 | train_loss: 0.0008 | val_loss: 0.0006 | test_loss: 0.0006:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:02:17<2:25:16, 622.63s/it]# Epoch 6 | train_loss: 0.0006 | val_loss: 0.0005 | test_loss: 0.0005:  30%|‚ñà‚ñà‚ñà       | 6/20 [1:12:48<2:25:16, 622.63s/it]# Epoch 6 | train_loss: 0.0006 | val_loss: 0.0005 | test_loss: 0.0005:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:12:48<2:15:30, 625.41s/it]# Epoch 7 | train_loss: 0.0004 | val_loss: 0.0004 | test_loss: 0.0004:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:22:11<2:15:30, 625.41s/it]# Epoch 7 | train_loss: 0.0004 | val_loss: 0.0004 | test_loss: 0.0004:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:22:11<2:01:06, 605.55s/it]# Epoch 8 | train_loss: 0.0004 | val_loss: 0.0003 | test_loss: 0.0003:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:32:44<2:01:06, 605.55s/it]# Epoch 8 | train_loss: 0.0004 | val_loss: 0.0003 | test_loss: 0.0003:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:32:44<1:52:35, 614.13s/it]# Epoch 9 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:43:17<1:52:35, 614.13s/it]# Epoch 9 | train_loss: 0.0003 | val_loss: 0.0003 | test_loss: 0.0003:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:43:17<1:43:18, 619.89s/it]# Epoch 10 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:53:49<1:43:18, 619.89s/it]# Epoch 10 | train_loss: 0.0003 | val_loss: 0.0002 | test_loss: 0.0002:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [1:53:49<1:33:34, 623.84s/it]# Epoch 11 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:04:23<1:33:34, 623.84s/it]# Epoch 11 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:04:23<1:23:34, 626.81s/it]# Epoch 12 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:14:57<1:23:34, 626.81s/it]# Epoch 12 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:14:57<1:13:23, 629.05s/it]# Epoch 13 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:25:31<1:13:23, 629.05s/it]# Epoch 13 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:25:31<1:03:03, 630.61s/it]# Epoch 14 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:36:07<1:03:03, 630.61s/it]# Epoch 14 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:36:07<52:40, 632.16s/it]  # Epoch 15 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:46:42<52:40, 632.16s/it]# Epoch 15 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [2:46:42<42:11, 632.89s/it]# Epoch 16 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [2:57:08<42:11, 632.89s/it]# Epoch 16 | train_loss: 0.0002 | val_loss: 0.0002 | test_loss: 0.0002:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [2:57:08<31:32, 630.99s/it]# Epoch 17 | train_loss: 0.0002 | val_loss: 0.0001 | test_loss: 0.0001:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:07:44<31:32, 630.99s/it]# Epoch 17 | train_loss: 0.0002 | val_loss: 0.0001 | test_loss: 0.0001:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:07:44<21:04, 632.32s/it]# Epoch 18 | train_loss: 0.0002 | val_loss: 0.0001 | test_loss: 0.0001:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:18:20<21:04, 632.32s/it]# Epoch 18 | train_loss: 0.0002 | val_loss: 0.0001 | test_loss: 0.0001:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:18:20<10:33, 633.51s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0001 | test_loss: 0.0001:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:28:52<10:33, 633.51s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0001 | test_loss: 0.0001: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:28:52<00:00, 633.13s/it]# Epoch 19 | train_loss: 0.0002 | val_loss: 0.0001 | test_loss: 0.0001: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:28:52<00:00, 626.64s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:  test_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 19
wandb:  test_loss 0.00014
wandb: train_loss 0.00016
wandb:   val_loss 0.00014
wandb: 
wandb: üöÄ View run autumn-dew-59 at: https://wandb.ai/yussufwaly/digital_twin_graphmae/runs/worj21bc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230724_100033-worj21bc/logs
./GraphMAE/embed.py:93: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
./GraphMAE/embed.py:93: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
./GraphMAE/embed.py:93: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
./GraphMAE/embed.py:93: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)
./GraphMAE/embed.py:93: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)
  dgl_graph.ndata['feat'] = torch.tensor(points, dtype=torch.float32)

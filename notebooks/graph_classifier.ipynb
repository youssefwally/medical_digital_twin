{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2023-11-10 09:57:41,153 - instantiator - Created a temporary directory at /tmp/tmp9ttna07b\n",
      "INFO - 2023-11-10 09:57:41,155 - instantiator - Writing /tmp/tmp9ttna07b/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "import pickle as pkl\n",
    "from itertools import tee\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, Linear, LayerNorm\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_sparse import SparseTensor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../../../../../../vol/aimspace/users/wyo/registered_meshes/2000/\"\n",
    "organ=\"liver_mesh.ply\"\n",
    "output=False\n",
    "save=False\n",
    "    \n",
    "    \n",
    "# train=True\n",
    "epochs=1\n",
    "batchs=4\n",
    "use_input_encoder=True\n",
    "in_features=3\n",
    "encoder_features=516\n",
    "hidden_channels=[1024, 1024, 512, 512, 256, 256, 256]\n",
    "num_conv_layers=4\n",
    "num_classes=1\n",
    "activation=\"ELU\"\n",
    "normalization=True\n",
    "layer=\"gat\"\n",
    "dropout=0.005\n",
    "lr=0.00007\n",
    "weight_decay=0.002\n",
    "optimizer=\"adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, organ, wanted_label='disease'):\n",
    "    registered_mesh = []\n",
    "    labels_path = \"../data/liver_diseases.csv\"\n",
    "    train_ids_path = \"../data/NonNa_organs_split_train.txt\"\n",
    "    val_ids_path = \"../data/NonNa_organs_split_val.txt\"\n",
    "    test_ids_path = \"../data/NonNa_organs_split_test.txt\"\n",
    "    labels = pd.read_csv(labels_path, delimiter=\",\", dtype=str, index_col=0)    \n",
    "    train_dirs = np.loadtxt(train_ids_path, delimiter=\",\", dtype=str)\n",
    "    val_dirs = np.loadtxt(val_ids_path, delimiter=\",\", dtype=str)\n",
    "    test_dirs = np.loadtxt(test_ids_path, delimiter=\",\", dtype=str)\n",
    "    dirs = next(os.walk(path))[1]\n",
    "    train_dataset = []\n",
    "    val_dataset = []\n",
    "    test_dataset = []\n",
    "    errors = []\n",
    "    #In Test\n",
    "    dirs = dirs[:5]\n",
    "    print(f'Number of samples used: {len(dirs)}', flush=True)\n",
    "\n",
    "    body_fields = [\"eid\", \"22407-2.0\", \"22408-2.0\", \"31-0.0\"]\n",
    "    full_ukbb_data = pd.read_csv(\"../../../../../../vol/aimspace/projects/ukbb/data/tabular/ukb668815_imaging.csv\", usecols=body_fields)\n",
    "    full_ukbb_data_new_names = {'22407-2.0':'VAT', '22408-2.0':'ASAT', '31-0.0':'sex'}\n",
    "    full_ukbb_data = full_ukbb_data.rename(index=str, columns=full_ukbb_data_new_names)\n",
    "    \n",
    "    basic_features = pd.read_csv(\"../data/basic_features.csv\")\n",
    "    basic_features_new_names = {'21003-2.0':'age', '31-0.0':'sex', '21001-2.0':'bmi', '21002-2.0':'weight','50-2.0':'height'}\n",
    "    basic_features = basic_features.rename(index=str, columns=basic_features_new_names)\n",
    "    print(f'Number of samples used: {len(dirs)}, with label: {wanted_label}', flush=True)\n",
    "\n",
    "    if(wanted_label == 'sex' or wanted_label == 'VAT' or wanted_label == 'ASAT'):\n",
    "        features = full_ukbb_data\n",
    "    else:\n",
    "        features = basic_features\n",
    "    \n",
    "    for dir in dirs:\n",
    "        registered_mesh = []\n",
    "        try: \n",
    "            mesh = o3d.io.read_triangle_mesh(f'{path}{dir}/{organ}')\n",
    "            vertices_data = np.asarray(mesh.vertices)\n",
    "            triangles = np.asarray(mesh.triangles)\n",
    "            vertices = torch.from_numpy(vertices_data).double()\n",
    "            edges = []\n",
    "            for triangle in triangles:\n",
    "                edges.append([triangle[0], triangle[1]])\n",
    "                edges.append([triangle[0], triangle[2]])\n",
    "                edges.append([triangle[1], triangle[2]])\n",
    "            edges_torch = [[],[]]\n",
    "            edges = np.unique(np.array(edges), axis=0)\n",
    "            for edge in edges:\n",
    "                edges_torch[0].append(edge[0])\n",
    "                edges_torch[1].append(edge[1])\n",
    "            edges_torch = torch.from_numpy(np.asarray(edges_torch)).long()\n",
    "            registered_mesh.append((vertices.type(torch.float32), edges_torch))\n",
    "\n",
    "            # label = labels.loc[int(dir)].to_list()\n",
    "            # label = [int(id) for id in label]\n",
    "            # label_np = np.asarray(label)\n",
    "            # label_tensor = torch.from_numpy(label_np)\n",
    "\n",
    "            if(wanted_label == 'disease'):\n",
    "                label = 1 if int(dir) in labels.index else 0\n",
    "                label_tensor = label\n",
    "                data = Data(x=registered_mesh[0][0], y=label_tensor, edge_index=registered_mesh[0][1], num_nodes= len(registered_mesh[0][0]))\n",
    "                if(dir in train_dirs):\n",
    "                    train_dataset.append(data)\n",
    "                if(dir in val_dirs):\n",
    "                    val_dataset.append(data)\n",
    "                elif(dir in test_dirs):\n",
    "                    test_dataset.append(data)\n",
    "            else:\n",
    "                cur_patient_feature = features[features['eid'] == int(dir)]\n",
    "                if(len(cur_patient_feature[wanted_label]) == 1):\n",
    "                    if(not pd.isnull(cur_patient_feature[wanted_label].item())):\n",
    "                        cur_patient_feature_tensor = torch.tensor(cur_patient_feature[wanted_label].item())\n",
    "                        data = Data(x=registered_mesh[0][0], y=cur_patient_feature_tensor, edge_index=registered_mesh[0][1], num_nodes= len(registered_mesh[0][0]))\n",
    "                        if(dir in train_dirs):\n",
    "                            train_dataset.append(data)\n",
    "                        if(dir in val_dirs):\n",
    "                            val_dataset.append(data)\n",
    "                        elif(dir in test_dirs):\n",
    "                            test_dataset.append(data)\n",
    "\n",
    "        except:\n",
    "            errors.append(dir)\n",
    "            \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "############################################################################################\n",
    "#Generating GNN layers\n",
    "def get_gnn_layers(num_conv_layers: int, hidden_channels, num_inp_features:int, \n",
    "                 gnn_layer, activation=nn.ReLU, normalization=None, dropout = None):\n",
    "    \"\"\"Creates GNN layers\"\"\"\n",
    "    layers = nn.ModuleList()\n",
    "    for i in range(num_conv_layers):\n",
    "        if i == 0:\n",
    "            layers.append(gnn_layer(num_inp_features, hidden_channels[i]))\n",
    "            layers.append(activation())\n",
    "            if normalization is not None:\n",
    "                layers.append(normalization(hidden_channels[i]))\n",
    "        else:\n",
    "            layers.append(gnn_layer(hidden_channels[i-1], hidden_channels[i]))\n",
    "            layers.append(activation())\n",
    "            if normalization is not None:\n",
    "                layers.append(normalization(hidden_channels[i]))\n",
    "    return nn.ModuleList(layers)\n",
    "    \n",
    "############################################################################################\n",
    "#Making multilayer perceptron layers \n",
    "def get_mlp_layers(channels: list, activation, output_activation=nn.Identity):\n",
    "    \"\"\"Define basic multilayered perceptron network.\"\"\"\n",
    "    layers = []\n",
    "    *intermediate_layer_definitions, final_layer_definition = pairwise(channels)\n",
    "    for in_ch, out_ch in intermediate_layer_definitions:\n",
    "        intermediate_layer = nn.Linear(in_ch, out_ch)\n",
    "        layers += [intermediate_layer, activation()]\n",
    "    layers += [nn.Linear(*final_layer_definition), output_activation()]\n",
    "    #print('Output activation ',output_activation)\n",
    "    return nn.Sequential(*layers)\n",
    "    \n",
    "############################################################################################\n",
    "#Iterate over all pairs of consecutive items in a list\n",
    "def pairwise(iterable):\n",
    "    \"\"\"Iterate over all pairs of consecutive items in a list.\n",
    "    Notes\n",
    "    -----\n",
    "        [s0, s1, s2, s3, ...] -> (s0,s1), (s1,s2), (s2, s3), ...\n",
    "    \"\"\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "    \n",
    "############################################################################################\n",
    "\n",
    "def reduce_loss(loss, reduction):\n",
    "    # none: 0, elementwise_mean:1, sum: 2\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss.sum()\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
    "    # if weight is specified, apply element-wise weight\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    # if avg_factor is not specified, just reduce the loss\n",
    "    if avg_factor is None:\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    else:\n",
    "        # if reduction is mean, then average the loss by avg_factor\n",
    "        if reduction == 'mean':\n",
    "            # Avoid causing ZeroDivisionError when avg_factor is 0.0,\n",
    "            # i.e., all labels of an image belong to ignore index.\n",
    "            eps = torch.finfo(torch.float32).eps\n",
    "            loss = loss.sum() / (avg_factor + eps)\n",
    "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
    "        elif reduction != 'none':\n",
    "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
    "    return loss\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def focal_loss(pred, target, alpha = 1, gamma = 2, weight=None, reduction='mean', avg_factor=None):\n",
    "    target = target.type_as(pred)\n",
    "    pt = (1 - pred) * target + pred * (1 - target)\n",
    "    focal_weight = (alpha * target + (1 - alpha) *\n",
    "                    (1 - target)) * pt.pow(gamma)\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        pred, target, reduction='none') * focal_weight\n",
    "    \n",
    "    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n",
    "    return loss\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "#Train function\n",
    "def train(model, optimizer, dataloader, alpha, gamma, threshold = 0.5, loss_fn = nn.BCEWithLogitsLoss()):\n",
    "    \"\"\"Train network on training dataset.\"\"\"\n",
    "    model.train()\n",
    "    cumulative_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_fn(out.squeeze(1), data.y.float())\n",
    "        # print(f\"output: {out.squeeze(1)}, gt: {data.y.float()}, loss: {loss}\", flush=True)\n",
    "        loss.backward()\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return cumulative_loss / len(dataloader)\n",
    "    # model.train()\n",
    "    # cumulative_loss = 0.0\n",
    "    # for data in dataloader:\n",
    "    #     data = data.to(device)      \n",
    "    #     weights = torch.where(data.y.float() == 1.0, torch.tensor(5), torch.tensor(1))  \n",
    "\n",
    "    #     output = model(data) \n",
    "    #     output = output.squeeze(1).flatten()\n",
    "    #     prediction = (output > threshold).float() \n",
    "    #     prediction = torch.tensor(prediction, requires_grad=True)\n",
    "    #     intermediate_losses = loss_fn(prediction, data.y.float())\n",
    "    #     loss = torch.mean(weights*intermediate_losses)\n",
    "    #     # loss = focal_loss(output, data.y.float(), alpha = alpha, gamma = gamma, weight=None, reduction='mean', avg_factor=None)\n",
    "    #     cumulative_loss += loss.item()\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    # return cumulative_loss / len(dataloader)\n",
    "    \n",
    "############################################################################################\n",
    "\n",
    "#Validation function\n",
    "def calculate_val_loss(model, dataloader, alpha, gamma, threshold = 0.5, loss_fn = nn.BCEWithLogitsLoss()):\n",
    "    model.eval()\n",
    "    cumulative_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = loss_fn(out.squeeze(1), data.y.float())\n",
    "        cumulative_loss += loss.item()\n",
    "    return cumulative_loss / len(dataloader)\n",
    "    # model.eval()\n",
    "    # cumulative_loss = 0.0\n",
    "    # for data in dataloader:\n",
    "    #     data = data.to(device)        \n",
    "    #     weights = torch.where(data.y.float() == 1.0, torch.tensor(5), torch.tensor(1))    \n",
    "\n",
    "    #     output = model(data) \n",
    "    #     output = output.squeeze(1).flatten()\n",
    "    #     prediction = (output > threshold).float() \n",
    "    #     prediction = torch.tensor(prediction, requires_grad=True)\n",
    "    #     intermediate_losses = loss_fn(prediction, data.y.float())\n",
    "    #     loss = torch.mean(weights*intermediate_losses)\n",
    "    #     # loss = focal_loss(output, data.y.float(), alpha = alpha, gamma = gamma, weight=None, reduction='mean', avg_factor=None)\n",
    "    #     cumulative_loss += loss.item()\n",
    "    \n",
    "    # return cumulative_loss / len(dataloader)\n",
    "    \n",
    "############################################################################################\n",
    "\n",
    "#Test function\n",
    "def test(model, dataloader, alpha, gamma, threshold = 0.5, loss_fn = nn.BCEWithLogitsLoss()):    \n",
    "    model.eval()\n",
    "    prediction_accuracies = []\n",
    "    prediction_f1 = []\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        predictions = model(data)\n",
    "        # predicted_class_labels = torch.nn.Sigmoid()(predictions)\n",
    "        predicted_class_labels = predictions.squeeze(1)\n",
    "        predicted_class_labels = torch.round(predicted_class_labels)\n",
    "\n",
    "        correct_assignments = (predicted_class_labels == data.y.float()).sum()\n",
    "        num_assignemnts = predicted_class_labels.shape[0]\n",
    "        # print(f\"output: {predictions}, pred: {predicted_class_labels}, gt: {data.y.float()}, correct_assignments: {correct_assignments}, num_assignemnts: {num_assignemnts}, Acc: {float(correct_assignments / num_assignemnts)}\", flush=True)\n",
    "        prediction_accuracies.append(float(correct_assignments / num_assignemnts))\n",
    "        f1_score = sklearn.metrics.precision_score(predicted_class_labels.int().detach().cpu(), data.y.int().detach().cpu(), average='weighted')\n",
    "        prediction_f1.append((f1_score))\n",
    "    print(prediction_f1)\n",
    "    return sum(prediction_accuracies) / len(dataloader), sum(prediction_f1) / len(dataloader)\n",
    "    # model.eval()\n",
    "    # cumulative_loss = 0.0\n",
    "    # correct = 0\n",
    "    # total = 0\n",
    "    # for data in dataloader:\n",
    "    #     data = data.to(device)        \n",
    "    #     weights = torch.where(data.y.float() == 1.0, torch.tensor(5), torch.tensor(1))  \n",
    "\n",
    "    #     output = model(data)\n",
    "    #     output = output.squeeze(1).flatten()\n",
    "    #     prediction = (output > threshold).float() \n",
    "    #     prediction = torch.tensor(prediction, requires_grad=True)\n",
    "    #     intermediate_losses = loss_fn(prediction, data.y.float())\n",
    "    #     print(f\"pred: {prediction}, gt: {data.y.float()}, in_losses: {intermediate_losses}, final losses: {torch.mean(weights*intermediate_losses)}\")\n",
    "    #     loss = torch.mean(weights*intermediate_losses)\n",
    "    #     # loss = focal_loss(prediction, data.y.float(), alpha = alpha, gamma = gamma, weight=None, reduction='mean', avg_factor=None)\n",
    "    #     cumulative_loss += loss.item()\n",
    "\n",
    "    #     correct += (prediction == data.y).sum().item()\n",
    "    #     total += data.y.size(0)\n",
    "    \n",
    "    # loss = cumulative_loss / len(dataloader)\n",
    "    # accuracy = correct / total\n",
    "    # precision = sklearn.metrics.precision_score(prediction.int(), data.y.int())\n",
    "    # recall = sklearn.metrics.recall_score(prediction.int(), data.y.int())\n",
    "    # f1_score = sklearn.metrics.f1_score(prediction.int(), data.y.int())\n",
    "    # ap = sklearn.metrics.average_precision_score(prediction.int(), data.y.int())\n",
    "    # auc = sklearn.metrics.auc(prediction.int(), data.y.int())\n",
    "    # return loss, accuracy, precision, recall, f1_score, ap, auc\n",
    "    \n",
    "############################################################################################\n",
    "\n",
    "#Optimizer\n",
    "def build_optimizer(network, optimizer, learning_rate, weight_decay):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "    \n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_channels, activation, normalization, num_classes, num_conv_layers=4, layer='gcn',\n",
    "                 use_input_encoder=True, encoder_features=128, apply_batch_norm=True,\n",
    "                 apply_dropout_every=True, dropout = 0):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        self.fc = torch.nn.ModuleList()\n",
    "        self.layer_type = layer\n",
    "        self.num_classes = num_classes\n",
    "        self.use_input_encoder = use_input_encoder\n",
    "        self.apply_batch_norm = apply_batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.normalization_bool = normalization\n",
    "        self.activation = activation\n",
    "        self.apply_dropout_every = apply_dropout_every\n",
    "\n",
    "        if self.normalization_bool:\n",
    "            self.normalization = LayerNorm\n",
    "        else:\n",
    "            self.normalization = None\n",
    "\n",
    "        if self.use_input_encoder :\n",
    "            self.input_encoder = get_mlp_layers(\n",
    "                channels=[in_features, encoder_features],\n",
    "                activation=nn.ELU,\n",
    "            )\n",
    "            in_features = encoder_features\n",
    "\n",
    "        if layer == 'gcn':\n",
    "            self.layers = get_gnn_layers(num_conv_layers, hidden_channels, num_inp_features=in_features,\n",
    "                                        gnn_layer=GCNConv,activation=activation,normalization=self.normalization )\n",
    "        elif layer == 'sageconv':\n",
    "            self.layers = get_gnn_layers(num_conv_layers, hidden_channels,in_features,\n",
    "                                        gnn_layer=SAGEConv,activation=activation,normalization=self.normalization )\n",
    "        elif layer == 'gat':\n",
    "            self.layers = get_gnn_layers(num_conv_layers, hidden_channels,in_features,\n",
    "                                        gnn_layer=GATConv,activation=activation,normalization=self.normalization )     \n",
    "\n",
    "        for i in range((len(hidden_channels)-num_conv_layers)):\n",
    "            self.fc.append(Linear(hidden_channels[i+num_conv_layers-1], hidden_channels[i+num_conv_layers]))\n",
    "        \n",
    "        self.pred_layer = Linear(hidden_channels[len(hidden_channels)-1], self.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        if self.use_input_encoder:\n",
    "            x = self.input_encoder(x)\n",
    "\n",
    "        if self.normalization is None:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                # Each GCN consists 2 modules GCN -> Activation \n",
    "                # GCN send edge index\n",
    "                if i% 2 == 0:\n",
    "                    x = layer(x, edge_index)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "\n",
    "                if self.apply_dropout_every:\n",
    "                    x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        else:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                # Each GCN consists 3 modules GCN -> Activation ->  Normalization \n",
    "                # GCN send edge index\n",
    "                if i% 3 == 0:\n",
    "                    x = layer(x, edge_index)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "\n",
    "                if self.apply_dropout_every:\n",
    "                    x = F.dropout(x, p=self.dropout, training=self.training)      \n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_max_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "       \n",
    "        for i in range(len(self.fc)):\n",
    "           x = self.fc[i](x)\n",
    "           x = torch.tanh(x)\n",
    "           x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.pred_layer(x)  \n",
    "        x = torch.nn.Sigmoid()(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used: 382, 62, 40\n",
      "Original Distrubution: [376, 6, 60, 2, 39, 1]\n",
      "Used Distrubution: [6, 6, 2, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "torch_geometric.seed_everything(42)\n",
    "\n",
    "registeration_path = path\n",
    "\n",
    "#Model Parameters\n",
    "activation = torch.nn.modules.activation.ELU\n",
    "model_params = dict(\n",
    "        use_input_encoder = use_input_encoder,\n",
    "        in_features= in_features, \n",
    "        encoder_features = encoder_features,\n",
    "        hidden_channels= hidden_channels,\n",
    "        num_classes= num_classes,\n",
    "        activation=activation,\n",
    "        normalization = normalization,\n",
    "        layer = layer,\n",
    "        num_conv_layers = num_conv_layers,\n",
    "        dropout = dropout)\n",
    "\n",
    "    \n",
    "\n",
    "# model\n",
    "model = GNN(**model_params)\n",
    "\n",
    "# move to GPU (if available)\n",
    "device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_data(path, organ)\n",
    "\n",
    "healthy_train_dataset = []\n",
    "healthy_val_dataset = []\n",
    "healthy_test_dataset = []\n",
    "unhealthy_train_dataset = []\n",
    "unhealthy_val_dataset = []\n",
    "unhealthy_test_dataset = []\n",
    "\n",
    "count = [0,0,0,0,0,0]\n",
    "for i in train_dataset:\n",
    "    count[i.y] = count[i.y] + 1\n",
    "    if(i.y==0):\n",
    "        healthy_train_dataset.append(i)\n",
    "    else:\n",
    "        unhealthy_train_dataset.append(i)\n",
    "for i in val_dataset:\n",
    "    count[i.y+2] = count[i.y+2] + 1\n",
    "    if(i.y==0):\n",
    "        healthy_val_dataset.append(i)\n",
    "    else:\n",
    "        unhealthy_val_dataset.append(i)\n",
    "for i in test_dataset:\n",
    "    count[i.y+4] = count[i.y+4] + 1\n",
    "    if(i.y==0):\n",
    "        healthy_test_dataset.append(i)\n",
    "    else:\n",
    "        unhealthy_test_dataset.append(i)\n",
    "print(f\"Original Distrubution: {count}\", flush=True)\n",
    "\n",
    "healthy_train_dataset = random.sample(healthy_train_dataset, count[1])\n",
    "healthy_val_dataset = random.sample(healthy_val_dataset, count[3])\n",
    "healthy_test_dataset = random.sample(healthy_test_dataset, count[5])\n",
    "\n",
    "train_dataset = healthy_train_dataset + unhealthy_train_dataset\n",
    "val_dataset = healthy_val_dataset + unhealthy_val_dataset\n",
    "test_dataset = healthy_test_dataset + unhealthy_test_dataset\n",
    "\n",
    "count = [0,0,0,0,0,0]\n",
    "for i in train_dataset:\n",
    "    count[i.y] = count[i.y] + 1\n",
    "for i in val_dataset:\n",
    "    count[i.y+2] = count[i.y+2] + 1\n",
    "for i in test_dataset:\n",
    "    count[i.y+4] = count[i.y+4] + 1\n",
    "print(f\"Used Distrubution: {count}\", flush=True)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size=batchs, shuffle=True )\n",
    "valid_loader = DataLoader(dataset = val_dataset, batch_size=batchs, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size=batchs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n",
      "train_loss: 0.9571395715077718, val_loss: 0.9568238258361816, acc: 0.5, f1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# inizialize the optimizer\n",
    "optimizer = build_optimizer(model, optimizer, lr, weight_decay)\n",
    "alpha, gamma, threshold = 0.1, 2, 0.3\n",
    "\n",
    "# loss_fn = nn.BCELoss(reduction='none')\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0, device=device))\n",
    "    \n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, optimizer, train_loader, alpha, gamma, threshold, loss_fn)\n",
    "    val_loss = calculate_val_loss(model, valid_loader, alpha, gamma, threshold, loss_fn)\n",
    "\n",
    "    # test_loss, test_accuracy, p, r, f1, ap, auc = test(model, test_loader, alpha, gamma, threshold, loss_fn)\n",
    "    acc, f1_score = test(model, test_loader, alpha, gamma, threshold, loss_fn)\n",
    "\n",
    "    # print(f\"train_loss: {train_loss}, val_loss: {val_loss}, test_loss: {test_loss}, test_accuracy: {test_accuracy}, p: {p}, r: {r}, f1: {f1}, ap: {ap}, auc: {auc}\")\n",
    "    print(f\"train_loss: {train_loss}, val_loss: {val_loss}, acc: {acc}, f1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], dtype=torch.int32)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test_data = test_dataset[1]\n",
    "model.eval()\n",
    "output = model(test_data)\n",
    "\n",
    "output = output.squeeze(1).flatten()\n",
    "output = torch.nn.Sigmoid()(output)\n",
    "prediction = (output > threshold).float()  \n",
    "print(prediction.int())\n",
    "print(test_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 1})\n",
      "Counter({1: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(np.asarray(prediction)))\n",
    "print(Counter(np.asarray([test_data.y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 29891, 1: 488})\n"
     ]
    }
   ],
   "source": [
    "labels_path = \"../data/liver_diseases.csv\"\n",
    "labels = pd.read_csv(labels_path, delimiter=\",\", dtype=str, index_col=0) \n",
    "dirs = next(os.walk(path))[1]\n",
    "gt = []\n",
    "for dir in dirs:\n",
    "    label = 1 if int(dir) in labels.index else 0\n",
    "    gt.append(label)\n",
    "\n",
    "print(Counter(np.asarray(gt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6, 2, 2, 1, 1]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = [0,0,0,0,0,0]\n",
    "for i in train_dataset:\n",
    "    count[i.y] = count[i.y] + 1\n",
    "for i in val_dataset:\n",
    "    count[i.y+2] = count[i.y+2] + 1\n",
    "for i in test_dataset:\n",
    "    count[i.y+4] = count[i.y+4] + 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path=\"../../../../../../vol/aimspace/users/wyo/registered_meshes/2000/\", organ=\"liver_mesh.ply\", label=\"height\", save=False):\n",
    "    registered_mesh = []\n",
    "    test_ids_path = \"../data/NonNa_organs_split_test.txt\"\n",
    "    test_dirs = np.loadtxt(test_ids_path, delimiter=\",\", dtype=str)\n",
    "    dirs = next(os.walk(path))[1]\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "\n",
    "    #In Test\n",
    "    dirs = dirs[:5]\n",
    "    body_fields = [\"eid\", \"22407-2.0\", \"22408-2.0\", \"31-0.0\"]\n",
    "    full_ukbb_data = pd.read_csv(\"../../../../../../vol/aimspace/projects/ukbb/data/tabular/ukb668815_imaging.csv\", usecols=body_fields)\n",
    "    full_ukbb_data_new_names = {'22407-2.0':'VAT', '22408-2.0':'ASAT', '31-0.0':'sex'}\n",
    "    full_ukbb_data = full_ukbb_data.rename(index=str, columns=full_ukbb_data_new_names)\n",
    "    \n",
    "    basic_features = pd.read_csv(\"../data/basic_features.csv\")\n",
    "    basic_features_new_names = {'21003-2.0':'age', '31-0.0':'sex', '21001-2.0':'bmi', '21002-2.0':'weight','50-2.0':'height'}\n",
    "    basic_features = basic_features.rename(index=str, columns=basic_features_new_names)\n",
    "    print(f'Number of samples used: {len(dirs)}, with label: {label}', flush=True)\n",
    "\n",
    "    if(label == 'sex' or label == 'VAT' or label == 'ASAT'):\n",
    "        features = full_ukbb_data\n",
    "    else:\n",
    "        features = basic_features\n",
    "    \n",
    "    for dir in dirs:\n",
    "        registered_mesh = []\n",
    "        mesh = o3d.io.read_triangle_mesh(f'{path}{dir}/{organ}')\n",
    "    \n",
    "        vertices_data = np.asarray(mesh.vertices)\n",
    "        triangles = np.asarray(mesh.triangles)\n",
    "        vertices = torch.from_numpy(vertices_data).double()\n",
    "        edges = []\n",
    "        for triangle in triangles:\n",
    "            edges.append([triangle[0], triangle[1]])\n",
    "            edges.append([triangle[0], triangle[2]])\n",
    "            edges.append([triangle[1], triangle[2]])\n",
    "            \n",
    "        edges_torch = [[],[]]\n",
    "        edges =np.unique(np.array(edges), axis=0)\n",
    "        for edge in edges:\n",
    "            edges_torch[0].append(edge[0])\n",
    "            edges_torch[1].append(edge[1])\n",
    "    \n",
    "        edges_torch = torch.from_numpy(np.asarray(edges_torch)).long()\n",
    "\n",
    "        cur_patient_feature = features[features['eid'] == int(dir)]\n",
    "        if(len(cur_patient_feature[label]) == 1):\n",
    "            if(not pd.isnull(cur_patient_feature[label].item())):\n",
    "                cur_patient_feature_tensor = torch.tensor(cur_patient_feature[label].item())\n",
    "                registered_mesh.append((vertices.type(torch.float32), edges_torch, cur_patient_feature_tensor.type(torch.float32)))\n",
    "                data = Data(x=registered_mesh[0][0], y=registered_mesh[0][2], edge_index=registered_mesh[0][1], num_nodes= len(registered_mesh[0][0]))\n",
    "                print(data, flush=True)\n",
    "                if(dir in test_dirs):\n",
    "                    test_dataset.append(data)\n",
    "                else:\n",
    "                    train_dataset.append(data)\n",
    "                # data = train_test_split_edges(data)\n",
    "                # print(data)\n",
    "    \n",
    "    if(save):\n",
    "        with open(f'../data/infograph/{organ}/data', 'wb') as f:\n",
    "            pkl.dump(train_dataset, f)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used: 5, with label: height\n",
      "Data(x=[1087, 3], edge_index=[2, 4555], y=178.0, num_nodes=1087)\n",
      "Data(x=[1062, 3], edge_index=[2, 4504], y=188.0, num_nodes=1062)\n",
      "Data(x=[1091, 3], edge_index=[2, 4487], y=164.0, num_nodes=1091)\n",
      "Data(x=[1086, 3], edge_index=[2, 4567], y=181.0, num_nodes=1086)\n",
      "Data(x=[1086, 3], edge_index=[2, 4522], y=163.0, num_nodes=1086)\n"
     ]
    }
   ],
   "source": [
    "x, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digital_twin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
